<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "Optima", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Float Seg</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">FLOAT: Factorized Learning of Object Attributes for Improved <br> Multi-object Multi-part Scene Parsing</span>
		<table align=center width=900px>
			<table align=center width=900px>
				<tr>
					<br>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://rishubhsingh.github.io/">Rishubh Singh<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://www.linkedin.com/in/pranavmicro7/">Pranav Gupta<sup>2</sup></a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://sites.google.com/site/pshenoyuw/">Pradeep Shenoy<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://ravika.github.io/">Ravi Kiran Sarvadevabhatla<sup>2</sup></a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=900px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><sup>1</sup>Google Research</span>
						</center>
					</td>
					<td align=center width=600px>
						<center>
							<span style="font-size:24px"><sup>2</sup>International Institute of Information Technology, Hyderabad</span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=900px>
				<tr>
					<br>
					<td align=center width=600px>
						<center>
							<span style="font-size:24px">In <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a></span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=350px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='assets/float_arxiv_1.pdf'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/floatseg/floatseg.github.io'>[Code]</a></span><br>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://doi.org/10.5281/zenodo.6374908'>[Dataset]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<br>
				<td width=260px>
					<center>
						<img class="round" style="width:900px" src="./assets/figure1.jpg"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Multi-object multi-part semantic segmentation results for sample images from our expanded label space dataset, Pascal-Part-201. 
					Compared to state of the art BSANet, FLOAT accurately segments tiny parts (e.g. <i>left eyebrow</i>, <i>right eyebrow</i> on faces in 
					upper image) and handles scale variations better - note the size variations of <i>person</i> instances. Also, observe that FLOAT predicts 
					directional attributes of parts (e.g. 'left'/'right') accurately - ['left'/'right']: see <i>eyebrow</i>, <i>eye</i>, <i>arm</i> in upper image and <i>leg</i> 
					in lower image ; ['front'/back']: see <i>wheel</i> parts of the <i>bicycle</i> (lower image).
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Multi-object multi-part scene parsing is a challenging task which requires detecting multiple object classes in a scene and 
				segmenting the semantic parts within each object. In this paper, we propose FLOAT, a factorized label space framework for 
				scalable multi-object multi-part parsing. Our framework involves independent dense prediction of object category and part 
				attributes which increases scalability and reduces task complexity compared to the monolithic label space counterpart. 
				In addition, we propose an inference-time 'zoom' refinement technique which significantly improves segmentation quality, 
				especially for smaller objects/parts. Compared to state of the art, FLOAT obtains an absolute improvement of 2.0% for mean 
				IOU (mIOU) and 4.8% for segmentation quality IOU (sqIOU) on the Pascal-Part-58 dataset. For the larger Pascal-Part-108 dataset, 
				the improvements are 2.1% for mIOU and 3.9% for sqIOU. We incorporate previously excluded part attributes and other minor parts 
				of the Pascal-Part dataset to create the most comprehensive and challenging version which we dub Pascal-Part-201. FLOAT obtains 
				improvements of 8.6% for mIOU and 7.5% for sqIOU on the new dataset, demonstrating its parsing effectiveness across a 
				challenging diversity of objects and parts.
			</td>
		</tr>
	</table>
	<br>

	<!-- <hr> -->
	<!-- <center><h1>Talk</h1></center> -->
	<!-- <p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p> -->

	<!-- <table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table> -->
	<hr>

	<center><h1>Method</h1></center>

	<!-- <table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table> -->
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:800px" src="./assets/float.jpg"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					An overview diagram of our FLOAT framework (Sec. 3). Given an input image I, an object-level semantic segmentation network 
					(M<sub>obj</sub> , in blue) generates object prediction map (S<sub>o</sub>). Two decoders (in orange) produce object category grouped part-level prediction 
					maps for 'animate' (S<sub>a</sub>) and 'inanimate' objects (S<sub>i</sub>) in the scene. Another decoder (in red) produces part-attribute grouped prediction 
					maps for 'left-right' (S<sub>lr</sub>) and 'front-back' (S<sub>fb</sub>). At inference time (shown by dotted lines), outputs from the decoders are merged in a 
					top-down manner. The resulting prediction is further refined using the IZR technique (below) to obtain the final segmentation map (S<sub>p</sub>).
				</td>
			</tr>
		</center>
	</table>
	<br>
	<br>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:800px" src="./assets/izr.jpg"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					An overview of Inference-time Zoom Refinement (IZR). During inference, predictions from the object-level network 
					M<sub>obj</sub> are used to obtain padded bounding boxes for scene objects (B). The corresponding object crops (C) are processed by the factorized 
					network (F). The resulting label maps (D) are composited to generate S<sub>p</sub>, the final refined part segmentation map (E). Notice the 
					improvement in segmentation quality relative to the part label map without IZR (included for comparison).
				</td>
			</tr>
		</center>
	</table>
	<!-- <table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/richzhang/webpage-template'>[GitHub]</a>
			</center>
		</span>
	</table> -->
	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./assets/paper.png"/></a></td>
			<td><span style="font-size:14pt">R. Singh, P. Gupta, P. Shenoy, R. Sarvadevabhatla<br>
				<b>FLOAT: Factorized Learning of Object Attributes for Improved Multi-object Multi-part Scene Parsing</b><br>
				In Conference, CVPR 2022.<br>
				(hosted on <a href="assets/float_arxiv_1.pdf">ArXiv</a>)<br>
				<!-- (<a href="./assets/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<!-- <table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./assets/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table> -->

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<center>
					<!-- <center><h1>Acknowledgements</h1></center> -->
					<p style="font-size:15px;">This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.</p>
				</center>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

